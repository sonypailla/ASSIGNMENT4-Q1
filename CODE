import nltk

from nltk.tokenize import TreebankWordTokenizer

from nltk.corpus import stopwords

from nltk.stem import PorterStemmer


# Force download necessary resources

nltk.download('stopwords', force=True)


def nlp_preprocessing_pipeline(sentence):

tokenizer = TreebankWordTokenizer()


# Step 1: Tokenization (no 'punkt' needed)

tokens = tokenizer.tokenize(sentence)

print("Original Tokens:")

print(tokens)


# Step 2: Stopword Removal

stop_words = set(stopwords.words('english'))

filtered_tokens = [word for word in tokens if word.lower() not in stop_words]

print("\nTokens Without Stopwords:")

print(filtered_tokens)


# Step 3: Stemming

stemmer = PorterStemmer()

stemmed_words = [stemmer.stem(word) for word in filtered_tokens]

print("\nStemmed Words:")

print(stemmed_words)


# Example sentence

sentence = "NLP techniques are used in virtual assistants like Alexa and Siri."

nlp_preprocessing_pipeline(sentence)
